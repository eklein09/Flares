---
title: "Predicting Solar Flares from Sunspot Data"
author: "Ken Fenton, Eric Klein, Dave Lewis, Rob Patenge"
date: "November 28, 2015"
output: pdf_document
---

#Introduction and Motivation

In this paper, we use Machine Learning tools to develop a model for predicting solar flares using sunspot data. Solar flares strongly influence space weather in the vicinity of Earth, and can result in the production of high-energy particles - called a solar proton event - that can present radiation hazards to spacecraft and astronauts. X-rays and UV radiation from solar flares can also disrupt long-range radio communications and disturb the operation of radars and satellites. Therefore, it would be useful to predict solar flares at some point before the flare occurs (1).

**Data Set:** We began with two data sets (sunspot info and solar flare info) collected from the GOES Satellite Network ranging from 1996 through 2014.  The sunspot data included date, sunspot number, location, area, classification, longitudinal extent, number of spots, and magnetic classification.  The solar flare data included date, sunspot number the flare originated from, and flare intensity.  There are 21,778 sunspot observations to be matched against 13,203 C Class flares, 1,443 M Class flares, and 126 X class flares. We cleaned the data and joined the two sets (on the sunspot number) to consolidate into a single data set.

In the analysis below, we apply Gradient Boosting Machines, Random Forests, and several Deep Learning algorithms to develop a prediction model for solar flares based on the historical then. In addition, we added time series data (ROB - CAN YOU ELABORATE ON HOW YOU DID THIS?) to the basic data set and conducted further analysis in order to see if the data from a prior day improves our ability to predict a solar flare on the current day.

#Part I: Analysis without Time Series Data

```{r, echo=FALSE, include=FALSE}

#load libraries and data frame
library(randomForest)
library(gamlr)
library(ggplot2)
library(data.table)
library(h2o)

#update working directory for your machine
#setwd("/Users/Dave/Google Drive/Booth Academics")
flares_temp <- fread("1996_2014_data.csv")

flares <- flares_temp[, Sunspot_ID := NULL]

#set factor variables
flares[, Classification_Modified_Zurich := factor(Classification_Modified_Zurich)]
flares[, p_value := factor(p_value)]
flares[, c_value := factor(c_value)]
flares[, Magnetic_type := factor(Magnetic_type)]
flares[, Carrington_Longitude := as.integer(Carrington_Longitude)]

#assign threshold condition for Flare Intensity
flares[, isFlarePositive := Flare_Intensity>0 ]

toDelete <- colnames(flares)[17:26]
flares[, (toDelete) := NULL]
```

```{r, echo=FALSE, include=FALSE}

#configure h2o; break data into training and test
h2o <- h2o.init(nthreads = -1)
flares.hex <- as.h2o(flares, destination_frame = "flares.hex")

flares.split <- h2o.splitFrame(data = flares.hex,
                ratios = 0.8)
flares.training <- flares.split[[1]]
flares.test <- flares.split[[2]]
rm(flares.split)

#define x variables
x_vars <- setdiff(colnames(flares.hex), c("Flare_Intensity","isFlarePositive"))
```

##Gradient Boosting Machines

```{r, echo=FALSE, include=FALSE}

flares.gbm <- h2o.gbm(y="isFlarePositive",
                      x=x_vars,
                         training_frame=flares.training,
                         ntrees = 100,
                         max_depth = 5,
                         min_rows = 10,
                         learn_rate = 0.1,
                         distribution = "bernoulli")
```

```{r}
h2o.performance(model = flares.gbm, data = flares.test)
```


##Random Forests

```{r, echo=FALSE, include=FALSE}
flares.rf <- h2o.randomForest(y="isFlarePositive",
                      x=x_vars,
                         training_frame=flares.training,
                         ntrees = 100)
```

```{r, echo=FALSE}
h2o.performance(model = flares.rf, data = flares.test)
```

We see that GBM provides better performance, with an AUC of 0.85.

##Variable Importance

```{r, echo=FALSE}
flares.gbm@model$variable_importances
flares.rf@model$variable_importances
```

Unsurprisingly, we see that the Modified Zurich Classification is the most predictive variable. This makes sense from a physics standpoint because (KEN - CAN YOU ELABORATE?)

##Deep Learning

We now try several iterations of Deep Learning models with varying parameters.

###Default Model (Rectifier activation function, 2x200 hidden layers, no regularization, 1 epoch)

```{r, echo=TRUE, include = FALSE}
flares.dl <- h2o.deeplearning(
  y="isFlarePositive",
  x=x_vars,
  epochs=1,
  hidden = c(200,200),
  training_frame = flares.training)

flares.dl.performance <- h2o.performance(model = flares.dl, data = flares.test)

```

```{r}
flares.dl.performance@metrics$r2
flares.dl.performance@metrics$MSE
flares.dl.performance@metrics$logloss
```

R^2^ is 0.053 and MSE is 0.124 for the model with the default parameters.

###Tanh activation function

```{r, echo=FALSE, include=FALSE}
flares.dl2 <- h2o.deeplearning(
  y="isFlarePositive",
  x=x_vars,
  epochs=1,
  hidden = c(200,200),
  activation = "Tanh",
  training_frame = flares.training)

flares.dl2.performance <- h2o.performance(model = flares.dl2, data = flares.test)

```

```{r}
flares.dl2.performance@metrics$r2
flares.dl2.performance@metrics$MSE
flares.dl2.performance@metrics$logloss
```

R^2 improves significantly after using the Tanh activation function. R^2^ is 0.13 and MSE is 0.11.

###Tanh with dropout

Next, we try Tanh with dropout.

```{r, echo=FALSE, include=FALSE}
flares.dl3 <- h2o.deeplearning(
  y="isFlarePositive",
  x=x_vars,
  epochs=1,
  hidden = c(200,200),
  activation = "TanhWithDropout",
  training_frame = flares.training)

flares.dl3.performance <- h2o.performance(model = flares.dl3, data = flares.test)

```

```{r}
flares.dl3.performance@metrics$r2
flares.dl3.performance@metrics$MSE
flares.dl3.performance@metrics$logloss
```

Performance increases again using Dropout.R^2^ is 0.17 and MSE is 0.11.

###Three 100-node hidden layers

Next we see if we can improve performance by using a different series of hidden nodes.

```{r, echo=FALSE, include=FALSE}
flares.dl4 <- h2o.deeplearning(
  y="isFlarePositive",
  x=x_vars,
  epochs=1,
  hidden = c(100,100,100),
  activation = "TanhWithDropout",
  training_frame = flares.training)

flares.dl4.performance <- h2o.performance(model = flares.dl4, data = flares.test)

```

```{r}
flares.dl4.performance@metrics$r2
flares.dl4.performance@metrics$MSE
flares.dl4.performance@metrics$logloss
```

Performance is roughly the same using a 100-100-100 hidden layer.

###4 hidden layers of decreasing size

```{r, echo=FALSE, include = FALSE}
flares.dl5 <- h2o.deeplearning(
  y="isFlarePositive",
  x=x_vars,
  epochs=1,
  hidden = c(256,128,64,32),
  activation = "TanhWithDropout",
  training_frame = flares.training)

flares.dl5.performance <- h2o.performance(model = flares.dl5, data = flares.test)

```

```{r}
flares.dl5.performance@metrics$r2
flares.dl5.performance@metrics$MSE
flares.dl5.performance@metrics$logloss
```

This hidden layer structure fails to improve performance.

###Two 300-node hidden layers

We next shift back to two hidden layers and optimize the number of nodes in the hidden layer.


```{r, echo=FALSE, include=FALSE}
flares.dl6 <- h2o.deeplearning(
  y="isFlarePositive",
  x=x_vars,
  epochs=1,
  hidden = c(300,300),
  activation = "TanhWithDropout",
  training_frame = flares.training)

flares.dl6.performance <- h2o.performance(model = flares.dl6, data = flares.test)

```

```{r}
flares.dl6.performance@metrics$r2
flares.dl6.performance@metrics$MSE
flares.dl6.performance@metrics$logloss
```

Performance is significantly better using a 300-300 node hidden layer.

###10 epochs

Finally, we increase the number of epochs and see if performance improves.

```{r, echo=FALSE, include=FALSE}


if (!file.exists("C:\\Users\\eklein09\\Google Drive Booth\\BUS 41204 Machine Learning\\Final Project\\models\\DeepLearning_model_R_1449518005947_85")) {
  flares.dl7 <- h2o.deeplearning(
  y="isFlarePositive",
  x=x_vars,
  epochs=5,
  hidden = c(200,200),
  activation = "TanhWithDropout",
  training_frame = flares.training)
  h2o.saveModel(flares.dl7, path="models")
} else {
  flares.dl7 <- h2o.loadModel("C:\\Users\\eklein09\\Google Drive Booth\\BUS 41204 Machine Learning\\Final Project\\models\\DeepLearning_model_R_1449518005947_85")
}


flares.dl7.performance <- h2o.performance(model = flares.dl7, data = flares.test)

```

```{r}
flares.dl7.performance@metrics$r2
flares.dl7.performance@metrics$MSE
flares.dl7.performance@metrics$logloss
```

Deep Learning approaches the performance of GBM but takes much longer to train.

```{r, echo=FALSE}
flares.gbm.performance <- h2o.performance(model=flares.gbm,
                data=flares.test)@metrics$thresholds_and_metric_scores

flares.rf.performance <- h2o.performance(model=flares.rf,
                data=flares.test)@metrics$thresholds_and_metric_scores

flares.dl.performance <- h2o.performance(model=flares.dl7,
                data=flares.test)@metrics$thresholds_and_metric_scores

ggplot(data = flares.gbm.performance, aes(x=fpr, y= tpr, colour="gbm")) +
  geom_point(shape=1) +
geom_point(data = flares.rf.performance, aes(x=fpr, y= tpr, colour="rf")) +
 geom_point(data = flares.dl.performance, aes(x=fpr, y= tpr, colour="dl")) + 
  scale_colour_manual(values = c("gbm"="red", "rf" = "green","dl"="blue")) + ggtitle("Predictive Performance of Solar Flare Models\nwithout Lagged Data")
```

#Part II: Time Series Analysis

```{r, echo=FALSE, include=FALSE}

#update working directory for your machine
#setwd("/Users/Dave/Google Drive/Booth Academics")
flares_temp <- fread("lagData.csv")

flares <- flares_temp[, Sunspot_ID := NULL]

#set factor variables
flares <- flares[, Classification_Modified_Zurich := factor(Classification_Modified_Zurich)]
flares <- flares[, p_value := factor(p_value)]
flares <- flares[, c_value := factor(c_value)]
flares <- flares[, Magnetic_type := factor(Magnetic_type)]

#assign threshold condition for Flare Intensity
flares <- flares[, isFlarePositive := Flare_Intensity>0 ]

```

```{r, echo=FALSE, include=FALSE}

# configure h2o; break data into training and test
h2o <- h2o.init(nthreads = -1)
flares.hex <- as.h2o(flares, destination_frame = "flares.hex")

flares.split <- h2o.splitFrame(data = flares.hex,
                ratios = 0.8)
flares.training <- flares.split[[1]]
flares.test <- flares.split[[2]]
rm(flares.split)

#define x variables
x_vars <- setdiff(colnames(flares.hex), c("Flare_Intensity","isFlarePositive"))

```

##Gradient Boosting Machines

```{r, echo=FALSE, include=FALSE}

flares.gbm <- h2o.gbm(y="isFlarePositive",
                      x=x_vars,
                         training_frame=flares.training,
                         ntrees = 100,
                         max_depth = 5,
                         min_rows = 10,
                         learn_rate = 0.1,
                         distribution = "bernoulli")
```

```{r, echo=FALSE}
h2o.performance(flares.gbm, flares.test)
```

R^2^ is immediately significantly higher (0.52) than when the model does not contain lags. MSE is now 0.079.

##Random Forests

```{r, echo=FALSE, include=FALSE}
flares.rf <- h2o.randomForest(y="isFlarePositive",
                      x=x_vars,
                         training_frame=flares.training,
                         ntrees = 100)
```

```{r, echo=FALSE}
h2o.performance(flares.rf, flares.test)
```

Prediction error decreases further using RandomForests. R^2^ is 0.62 and SE is 0.064.

Adding time series data provides an increase in predictive performance, boosting AUC from 0.91 to 0.94.

##Variable Importance

```{r, echo=FALSE}
flares.gbm@model$variable_importances
flares.rf@model$variable_importances
```

Even with the time series data, the Modified Zurich Classification is still the most important variable for prediction, although lagged Flare Intensity data also has significant predictive value and likely accounts for the slight boost in performance.

##Deep Learning

Here again, we try several iterations of Deep learning models with the lagged data.

###Default Model (Rectifier activation function, 2x200 hidden layers, no regularization, 1 epoch)

```{r, echo=TRUE, include = FALSE}
flares.dl <- h2o.deeplearning(
  y="isFlarePositive",
  x=x_vars,
  epochs=1,
  hidden = c(200,200),
  training_frame = flares.training)

flares.dl.performance <- h2o.performance(model = flares.dl, data = flares.test)

```

```{r}
flares.dl.performance@metrics$r2
flares.dl.performance@metrics$MSE
flares.dl.performance@metrics$logloss
```

Performance for the deep learning network is significantly lower than for RF and GBM.

###Tanh activation function

```{r, echo=FALSE, include=FALSE}
flares.dl2 <- h2o.deeplearning(
  y="isFlarePositive",
  x=x_vars,
  epochs=1,
  hidden = c(200,200),
  activation = "Tanh",
  training_frame = flares.training)

flares.dl2.performance <- h2o.performance(model = flares.dl2, data = flares.test)

```

```{r}
flares.dl2.performance@metrics$r2
flares.dl2.performance@metrics$MSE
flares.dl2.performance@metrics$logloss
```

Tanh performs worse than Rectifier.

###Rectifier with dropout

```{r, echo=FALSE, include=FALSE}
flares.dl3 <- h2o.deeplearning(
  y="isFlarePositive",
  x=x_vars,
  epochs=1,
  hidden = c(200,200),
  activation = "RectifierWithDropout",
  training_frame = flares.training)

flares.dl3.performance <- h2o.performance(model = flares.dl3, data = flares.test)

```

```{r}
flares.dl3.performance@metrics$r2
flares.dl3.performance@metrics$MSE
flares.dl3.performance@metrics$logloss
```

Using dropout reduces the performance of the model.

###Three 100-node hidden layers

Next we see if we can improve performance by using a different series of hidden nodes.

```{r, echo=FALSE, include=FALSE}
flares.dl4 <- h2o.deeplearning(
  y="isFlarePositive",
  x=x_vars,
  epochs=1,
  hidden = c(100,100,100),
  activation = "Rectifier",
  training_frame = flares.training)

flares.dl4.performance <- h2o.performance(model = flares.dl4, data = flares.test)

```

```{r}
flares.dl4.performance@metrics$r2
flares.dl4.performance@metrics$MSE
flares.dl4.performance@metrics$logloss
```

Using three, 100-node hidden layers leads to worse performance than two, 200-node hidden layers.

###4 hidden layers of decreasing size

```{r, echo=FALSE, include = FALSE}
flares.dl5 <- h2o.deeplearning(
  y="isFlarePositive",
  x=x_vars,
  epochs=1,
  hidden = c(256,128,64,32),
  activation = "Rectifier",
  training_frame = flares.training)

flares.dl5.performance <- h2o.performance(model = flares.dl5, data = flares.test)

```

```{r}
flares.dl5.performance@metrics$r2
flares.dl5.performance@metrics$MSE
flares.dl5.performance@metrics$logloss
```

Layers of decreasing size does not beat two, 200-node hidden layers.

###Two 300-node hidden layers

We next shift back to two hidden layers and optimize the number of nodes in the hidden layer.


```{r, echo=FALSE, include=FALSE}
flares.dl6 <- h2o.deeplearning(
  y="isFlarePositive",
  x=x_vars,
  epochs=1,
  hidden = c(300,300),
  activation = "Rectifier",
  training_frame = flares.training)

flares.dl6.performance <- h2o.performance(model = flares.dl6, data = flares.test)

```

```{r}
flares.dl6.performance@metrics$r2
flares.dl6.performance@metrics$MSE
flares.dl6.performance@metrics$logloss
```

Increasing the size of the hidden layers offers only a small improvment over the original model.

###10 epochs

Finally, we increase the number of epochs and see if performance improves.

```{r, echo=FALSE, include=FALSE}


if (!file.exists("C:\\Users\\eklein09\\Google Drive Booth\\BUS 41204 Machine Learning\\Final Project\\lagmodels\\DeepLearning_model_R_1449518005947_130")) {
  flares.dl7 <- h2o.deeplearning(
  y="isFlarePositive",
  x=x_vars,
  epochs=5,
  hidden = c(200,200),
  activation = "Rectifier",
  training_frame = flares.training)
  h2o.saveModel(flares.dl7, path="lagmodels")
} else {
  flares.dl7 <- h2o.loadModel("C:\\Users\\eklein09\\Google Drive Booth\\BUS 41204 Machine Learning\\Final Project\\lagmodels\\DeepLearning_model_R_1449518005947_130")
}


flares.dl7.performance <- h2o.performance(model = flares.dl7, data = flares.test)

```

```{r}
flares.dl7.performance@metrics$r2
flares.dl7.performance@metrics$MSE
flares.dl7.performance@metrics$logloss
```

Once again, Deep Learning approaches the performance of GBM but takes much longer to train.

```{r, echo=FALSE}
flares.gbm.performance <- h2o.performance(model=flares.gbm,
                data=flares.test)@metrics$thresholds_and_metric_scores

flares.rf.performance <- h2o.performance(model=flares.rf,
                data=flares.test)@metrics$thresholds_and_metric_scores

flares.dl.performance <- h2o.performance(model=flares.dl7,
                data=flares.test)@metrics$thresholds_and_metric_scores

ggplot(data = flares.gbm.performance, aes(x=fpr, y= tpr, colour="gbm")) +
  geom_point(shape=1) +
geom_point(data = flares.rf.performance, aes(x=fpr, y= tpr, colour="rf")) +
 geom_point(data = flares.dl.performance, aes(x=fpr, y= tpr, colour="dl")) + 
  scale_colour_manual(values = c("gbm"="red", "rf" = "green","dl"="blue")) + ggtitle("Predictive Performance of Solar Flare Models\nwith Lagged Data")
```

# Conclusion

Our analysis shows that a GBM model provides the best performance for predicting solar flares. We believe this is because... 

--
(1)  Wikipedia contributors, "Solar flare," Wikipedia, The Free Encyclopedia, https://en.wikipedia.org/w/index.php?title=Solar_flare&oldid=689228024 (accessed November 8, 2015).